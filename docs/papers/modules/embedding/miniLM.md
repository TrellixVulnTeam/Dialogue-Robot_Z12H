# MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
+ https://github.com/microsoft/unilm/tree/master/minilm
+ https://arxiv.org/abs/2002.10957
+ https://www.msra.cn/zh-cn/news/features/minilm


# MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers