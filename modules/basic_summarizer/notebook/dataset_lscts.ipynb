{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:9: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:16: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:16: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_8276\\176903661.py:16: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    " \n",
    "lcsts_part_1=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
    "                           warn_bad_lines=True, error_bad_lines=False, sep='<[/d|/s|do|su|sh][^a].*>', encoding='utf-8') # PART I  too big, use PART II instead\n",
    "lcsts_part_1=lcsts_part_1[0].dropna()\n",
    "lcsts_part_1=lcsts_part_1.reset_index(drop=True)\n",
    "lcsts_part_1=pd.concat([lcsts_part_1[1::2].reset_index(drop=True), lcsts_part_1[::2].reset_index(drop=True)], axis=1)\n",
    "lcsts_part_1.columns=['document', 'summary']\n",
    " \n",
    "lcsts_part_2=pd.read_table('../../../resources/corpus/LCSTS_ORIGIN/DATA/PART_II.txt', header=None,\n",
    "                           warn_bad_lines=True, error_bad_lines=False, sep='<[/d|/s|do|su|sh][^a].*>', encoding='utf-8')\n",
    "lcsts_part_2=lcsts_part_2[0].dropna()\n",
    "lcsts_part_2=lcsts_part_2.reset_index(drop=True)\n",
    "lcsts_part_2=pd.concat([lcsts_part_2[1::2].reset_index(drop=True), lcsts_part_2[::2].reset_index(drop=True)], axis=1)\n",
    "lcsts_part_2.columns=['document', 'summary']\n",
    " \n",
    "dataset_train = Dataset.from_dict(lcsts_part_1)\n",
    "dataset_valid = Dataset.from_dict(lcsts_part_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6634a564eeb44f49c1cb1be9c43036b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TokenModel = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(TokenModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(doc) for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    inputs = [str(doc) for doc in examples[\"summary\"]]\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(inputs, max_length=max_target_length, truncation=True)\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77dcfd336aa46bcbae9c0a19bb192cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f049ff47d8064c48900b3721db1ab322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10666\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10666\n",
      "    })\n",
      "})\n",
      "{'document': ['日前，方舟子发文直指林志颖旗下爱碧丽推销假保健品，引起哗然。调查发现，爱碧丽没有自己的生产加工厂。其胶原蛋白饮品无核心研发，全部代工生产。号称有“逆生长”功效的爱碧丽“梦幻奇迹限量组”售价高达1080元，实际成本仅为每瓶4元！', '韩方应对路径可以概括为：企业道歉担责；政府公正不护短；民间祈福关怀。他们深知形象的重要，竭力呵护企业品牌和国家形象。正如有评论，韩国“政府+企业+民众”三位一体式呵护韩国国家形象的“苦心经营”，的确有值得我们借鉴之处。', '63岁退休教师谢淑华，拉着人力板车，历时1年，走了2万4千里路，带着年过九旬的妈妈环游中国，完成了妈妈“一辈子在锅台边转，也想出去走走”的心愿。她说：“妈妈愿意出去走走，我就愿意拉着，孝心不能等，能走多远就走多远。', '昨天，包括工农中建交五大行在内的多家银行，不约而同地在官网发布公告称，它们的房地产贷款政策没有变化。多家银行表示，会支持居民购买首套住房。一名金融问题专家称，“目前房价不具备大涨大跌的基础，特别是一二线城市狂跌的可能性小。”', '广东4名律师致函中国民航局，要求其规定，航班起飞前要向乘客公布机组人员信息包括安全飞行时间、职业经历等。沪上业内人士认为，一般能被安排执飞任务的飞行员，均拥有民航认可的飞行资质。而责任心和使命感是无法通过飞行时间反映。', '任教五十年，钱理群在2012年教师节前夕宣布“告别教育”。从北大退休后，钱理群投身中学教育，试图“改变人心”，他以鲁迅自励，要在绝望中反抗，但基础教育十年试水，却令他收获“丰富的痛苦”。他说，—切不能为应试教育服务的教育根本无立足之地。', '①北京和上海户籍的游客可获得韩国多次签证；②“整容客”可以不经由韩国使领馆、直接在网上申请签证；③中泰免签的实施日期尚未敲定；④越南已向中国持通行证旅游的公民全面开放。', '12月12日，多家被立案稽查的沪市公司集体对外发布退市风险提示公告，*ST国创位列\"\"黑名单\"\"。目前证监会调查仍在进行，*ST国创尚未收到此次立案调查书面结论意见。一旦立案调查事项触及相关规定，公司股票将被实施退市风险警示。\"', '据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内', '12日，上周遭到极端分子恐怖袭击的《查理周刊》公布了“幸存者专刊”的封面，绿色的底色上，穆罕默德嘴角向下，手中举着一周来所有人都耳熟能详的口号：我是查理。这名伊斯兰教先知的头顶上还写着一句话：宽恕与原谅。该封面的作者上周因为迟到逃过一劫。'], 'summary': ['林志颖公司疑涉虚假营销无厂房无研发', '从韩亚航空事故看其应对路径', '女子用板车拉九旬老母环游中国1年走2万4千里', '银行集体发声：房贷政策没变', '四律师上书民航总局：起飞前应公布机长信息', '钱理群“告别教育”', '中国游客大增多国放宽签证', '信披违规外加业绩亏损*ST国创退市风险概率大增', '发改委反垄断调查小组突击调查奔驰上海办事处', '巴黎查理周刊最新一期封面（图）'], 'input_ids': [[101, 3189, 1184, 8024, 3175, 5660, 2094, 1355, 3152, 4684, 2900, 3360, 2562, 7577, 3186, 678, 4263, 4819, 714, 2972, 7218, 969, 924, 978, 1501, 8024, 2471, 6629, 1517, 4197, 511, 6444, 3389, 1355, 4385, 8024, 4263, 4819, 714, 3766, 3300, 5632, 2346, 4638, 4495, 772, 1217, 2339, 1322, 511, 1071, 5540, 1333, 6028, 4635, 7650, 1501, 3187, 3417, 2552, 4777, 1355, 8024, 1059, 6956, 807, 2339, 4495, 772, 511, 1384, 4917, 3300, 100, 6847, 4495, 7270, 100, 1216, 3126, 4638, 4263, 4819, 714, 100, 3457, 2404, 1936, 6839, 7361, 7030, 5299, 100, 1545, 817, 7770, 6809, 10680, 1039, 8024, 2141, 7354, 2768, 3315, 788, 711, 3680, 4486, 125, 1039, 8013, 102], [101, 7506, 3175, 2418, 2190, 6662, 2520, 1377, 809, 3519, 2886, 711, 8038, 821, 689, 6887, 3624, 2857, 6569, 8039, 3124, 2424, 1062, 3633, 679, 2844, 4764, 8039, 3696, 7313, 4857, 4886, 1068, 2577, 511, 800, 812, 3918, 4761, 2501, 6496, 4638, 7028, 6206, 8024, 4998, 1213, 1457, 2844, 821, 689, 1501, 4277, 1469, 1744, 2157, 2501, 6496, 511, 3633, 1963, 3300, 6397, 6389, 8024, 7506, 1744, 100, 3124, 2424, 116, 821, 689, 116, 3696, 830, 100, 676, 855, 671, 860, 2466, 1457, 2844, 7506, 1744, 1744, 2157, 2501, 6496, 4638, 100, 5736, 2552, 5307, 5852, 100, 8024, 4638, 4802, 3300, 966, 2533, 2769, 812, 955, 7063, 722, 1905, 511, 102], [101, 8381, 2259, 6842, 828, 3136, 2360, 6468, 3902, 1290, 8024, 2861, 4708, 782, 1213, 3352, 6756, 8024, 1325, 3198, 122, 2399, 8024, 6624, 749, 123, 674, 125, 1283, 7027, 6662, 8024, 2372, 4708, 2399, 6814, 736, 3194, 4638, 1968, 1968, 4384, 3952, 704, 1744, 8024, 2130, 2768, 749, 1968, 1968, 100, 671, 6777, 2094, 1762, 7222, 1378, 6804, 6760, 8024, 738, 2682, 1139, 1343, 6624, 6624, 100, 4638, 2552, 2703, 511, 1961, 6432, 8038, 100, 1968, 1968, 2703, 2692, 1139, 1343, 6624, 6624, 8024, 2769, 2218, 2703, 2692, 2861, 4708, 8024, 2105, 2552, 679, 5543, 5023, 8024, 5543, 6624, 1914, 6823, 2218, 6624, 1914, 6823, 511, 102], [101, 3219, 1921, 8024, 1259, 2886, 2339, 1093, 704, 2456, 769, 758, 1920, 6121, 1762, 1079, 4638, 1914, 2157, 7213, 6121, 8024, 679, 5276, 5445, 1398, 1765, 1762, 2135, 5381, 1355, 2357, 1062, 1440, 4917, 8024, 2124, 812, 4638, 2791, 1765, 772, 6587, 3621, 3124, 5032, 3766, 3300, 1359, 1265, 511, 1914, 2157, 7213, 6121, 6134, 4850, 8024, 833, 3118, 2898, 2233, 3696, 6579, 743, 7674, 1947, 857, 2791, 511, 671, 1399, 7032, 6084, 7309, 7579, 683, 2157, 4917, 8024, 100, 4680, 1184, 2791, 817, 679, 1072, 1906, 1920, 3885, 1920, 6649, 4638, 1825, 4794, 8024, 4294, 1166, 3221, 671, 753, 5296, 1814, 2356, 4312, 6649, 4638, 1377, 5543, 2595, 2207, 511, 100, 102], [101, 2408, 691, 125, 1399, 2526, 2360, 5636, 1141, 704, 1744, 3696, 5661, 2229, 8024, 6206, 3724, 1071, 6226, 2137, 8024, 5661, 4408, 6629, 7607, 1184, 6206, 1403, 733, 2145, 1062, 2357, 3322, 5299, 782, 1447, 928, 2622, 1259, 2886, 2128, 1059, 7607, 6121, 3198, 7313, 510, 5466, 689, 5307, 1325, 5023, 511, 3772, 677, 689, 1079, 782, 1894, 6371, 711, 8024, 671, 5663, 5543, 6158, 2128, 2961, 2809, 7607, 818, 1218, 4638, 7607, 6121, 1447, 8024, 1772, 2881, 3300, 3696, 5661, 6371, 1377, 4638, 7607, 6121, 6598, 6574, 511, 5445, 6569, 818, 2552, 1469, 886, 1462, 2697, 3221, 3187, 3791, 6858, 6814, 7607, 6121, 3198, 7313, 1353, 3216, 511, 102], [101, 818, 3136, 758, 1282, 2399, 8024, 7178, 4415, 5408, 1762, 8151, 2399, 3136, 2360, 5688, 1184, 1911, 2146, 2357, 100, 1440, 1166, 3136, 5509, 100, 511, 794, 1266, 1920, 6842, 828, 1400, 8024, 7178, 4415, 5408, 2832, 6716, 704, 2110, 3136, 5509, 8024, 6407, 1745, 100, 3121, 1359, 782, 2552, 100, 8024, 800, 809, 7826, 6813, 5632, 1225, 8024, 6206, 1762, 5318, 3307, 704, 1353, 2834, 8024, 852, 1825, 4794, 3136, 5509, 1282, 2399, 6407, 3717, 8024, 1316, 808, 800, 3119, 5815, 100, 705, 2168, 4638, 4578, 5736, 100, 511, 800, 6432, 8024, 100, 1147, 679, 5543, 711, 2418, 6407, 3136, 5509, 3302, 1218, 4638, 3136, 5509, 3418, 3315, 3187, 4989, 6639, 722, 1765, 511, 102], [101, 405, 1266, 776, 1469, 677, 3862, 2787, 5093, 4638, 3952, 2145, 1377, 5815, 2533, 7506, 1744, 1914, 3613, 5041, 6395, 8039, 406, 100, 3146, 2159, 2145, 100, 1377, 809, 679, 5307, 4507, 7506, 1744, 886, 7566, 7667, 510, 4684, 2970, 1762, 5381, 677, 4509, 6435, 5041, 6395, 8039, 407, 704, 3805, 1048, 5041, 4638, 2141, 3177, 3189, 3309, 2213, 3313, 3145, 2137, 8039, 408, 6632, 1298, 2347, 1403, 704, 1744, 2898, 6858, 6121, 6395, 3180, 3952, 4638, 1062, 3696, 1059, 7481, 2458, 3123, 511, 102], [101, 8110, 3299, 8110, 3189, 8024, 1914, 2157, 6158, 4989, 3428, 4942, 3389, 4638, 3772, 2356, 1062, 1385, 7415, 860, 2190, 1912, 1355, 2357, 6842, 2356, 7599, 7372, 2990, 4850, 1062, 1440, 8024, 115, 100, 1744, 1158, 855, 1154, 107, 107, 7946, 1399, 1296, 107, 107, 511, 4680, 1184, 6395, 4664, 833, 6444, 3389, 793, 1762, 6822, 6121, 8024, 115, 100, 1744, 1158, 2213, 3313, 3119, 1168, 3634, 3613, 4989, 3428, 6444, 3389, 741, 7481, 5310, 6389, 2692, 6224, 511, 671, 3190, 4989, 3428, 6444, 3389, 752, 7555, 6239, 1350, 4685, 1068, 6226, 2137, 8024, 1062, 1385, 5500, 4873, 2199, 6158, 2141, 3177, 6842, 2356, 7599, 7372, 6356, 4850, 511, 107, 102], [101, 2945, 2544, 928, 1062, 830, 1384, 100, 4518, 7481, 100, 2845, 6887, 8024, 125, 3189, 677, 1286, 8108, 4157, 2340, 1381, 8024, 704, 1744, 1355, 3121, 1999, 1353, 1797, 3171, 6444, 3389, 2207, 5299, 4960, 1140, 3389, 6393, 1944, 7720, 677, 3862, 1215, 752, 1905, 8024, 6444, 1357, 3144, 2945, 3332, 3160, 8024, 2400, 2190, 1914, 1399, 1944, 7720, 7770, 5052, 6822, 6121, 749, 5276, 6448, 511, 2779, 3632, 3219, 3189, 3241, 130, 4157, 8024, 1259, 2886, 1266, 776, 3449, 6612, 2548, 3172, 118, 1944, 7720, 7218, 1545, 3302, 1218, 3300, 7361, 1062, 1385, 691, 1277, 2600, 5307, 4415, 1762, 1079, 4638, 1914, 1399, 5052, 4415, 782, 1447, 793, 4522, 1762, 677, 3862, 1215, 1062, 2147, 1079, 102], [101, 8110, 3189, 8024, 677, 1453, 6901, 1168, 3353, 4999, 1146, 2094, 2607, 2587, 6159, 1140, 4638, 517, 3389, 4415, 1453, 1149, 518, 1062, 2357, 749, 100, 2401, 2100, 5442, 683, 1149, 100, 4638, 2196, 7481, 8024, 5344, 5682, 4638, 2419, 5682, 677, 8024, 4946, 5383, 7949, 2548, 1673, 6235, 1403, 678, 8024, 2797, 704, 715, 4708, 671, 1453, 3341, 2792, 3300, 782, 6963, 5455, 4225, 5543, 6422, 4638, 1366, 1384, 8038, 2769, 3221, 3389, 4415, 511, 6821, 1399, 823, 3172, 1065, 3136, 1044, 4761, 4638, 1928, 7553, 677, 6820, 1091, 4708, 671, 1368, 6413, 8038, 2160, 2609, 680, 1333, 6446, 511, 6421, 2196, 7481, 4638, 868, 5442, 677, 1453, 1728, 711, 6826, 1168, 6845, 6814, 671, 1223, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[101, 3360, 2562, 7577, 1062, 1385, 4542, 3868, 5994, 969, 5852, 7218, 3187, 1322, 2791, 3187, 4777, 1355, 102], [101, 794, 7506, 762, 5661, 4958, 752, 3125, 4692, 1071, 2418, 2190, 6662, 2520, 102], [101, 1957, 2094, 4500, 3352, 6756, 2861, 736, 3194, 5439, 3678, 4384, 3952, 704, 1744, 122, 2399, 6624, 123, 674, 125, 1283, 7027, 102], [101, 7213, 6121, 7415, 860, 1355, 1898, 8038, 2791, 6587, 3124, 5032, 3766, 1359, 102], [101, 1724, 2526, 2360, 677, 741, 3696, 5661, 2600, 2229, 8038, 6629, 7607, 1184, 2418, 1062, 2357, 3322, 7270, 928, 2622, 102], [101, 7178, 4415, 5408, 100, 1440, 1166, 3136, 5509, 100, 102], [101, 704, 1744, 3952, 2145, 1920, 1872, 1914, 1744, 3123, 2160, 5041, 6395, 102], [101, 928, 2847, 6824, 6226, 1912, 1217, 689, 5327, 755, 2938, 115, 100, 1744, 1158, 6842, 2356, 7599, 7372, 3519, 4372, 1920, 1872, 102], [101, 1355, 3121, 1999, 1353, 1797, 3171, 6444, 3389, 2207, 5299, 4960, 1140, 6444, 3389, 1944, 7720, 677, 3862, 1215, 752, 1905, 102], [101, 2349, 7944, 3389, 4415, 1453, 1149, 3297, 3173, 671, 3309, 2196, 7481, 8020, 1745, 8021, 102]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets_t = dataset_train.map(preprocess_function, batched=True)\n",
    "tokenized_datasets_v = dataset_valid.map(preprocess_function, batched=True)\n",
    " \n",
    "tokenized_datasets = datasets.DatasetDict({\"train\":tokenized_datasets_t,\"validation\": tokenized_datasets_v})\n",
    "print(tokenized_datasets)\n",
    "\n",
    "print(tokenized_datasets['train'][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
