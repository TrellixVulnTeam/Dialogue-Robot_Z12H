{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17144/2001870770.py:9: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_I.txt', header=None,\n",
      "/tmp/ipykernel_17144/2001870770.py:9: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_I.txt', header=None,\n",
      "/tmp/ipykernel_17144/2001870770.py:9: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_1=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_I.txt', header=None,\n",
      "/tmp/ipykernel_17144/2001870770.py:16: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_II.txt', header=None,\n",
      "/tmp/ipykernel_17144/2001870770.py:16: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_II.txt', header=None,\n",
      "/tmp/ipykernel_17144/2001870770.py:16: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  lcsts_part_2=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_II.txt', header=None,\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    " \n",
    "lcsts_part_1=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_I.txt', header=None,\n",
    "                           warn_bad_lines=True, error_bad_lines=False, sep='<[/d|/s|do|su|sh][^a].*>', encoding='utf-8') # PART I  too big, use PART II instead\n",
    "lcsts_part_1=lcsts_part_1[0].dropna()\n",
    "lcsts_part_1=lcsts_part_1.reset_index(drop=True)\n",
    "lcsts_part_1=pd.concat([lcsts_part_1[1::2].reset_index(drop=True), lcsts_part_1[::2].reset_index(drop=True)], axis=1)\n",
    "lcsts_part_1.columns=['document', 'summary']\n",
    " \n",
    "lcsts_part_2=pd.read_table('../../../resources/corpus/modules/sum/lscts/DATA/PART_II.txt', header=None,\n",
    "                           warn_bad_lines=True, error_bad_lines=False, sep='<[/d|/s|do|su|sh][^a].*>', encoding='utf-8')\n",
    "lcsts_part_2=lcsts_part_2[0].dropna()\n",
    "lcsts_part_2=lcsts_part_2.reset_index(drop=True)\n",
    "lcsts_part_2=pd.concat([lcsts_part_2[1::2].reset_index(drop=True), lcsts_part_2[::2].reset_index(drop=True)], axis=1)\n",
    "lcsts_part_2.columns=['document', 'summary']\n",
    " \n",
    "dataset_train = Dataset.from_dict(lcsts_part_1)\n",
    "dataset_valid = Dataset.from_dict(lcsts_part_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400591\n",
      "10666\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_train))\n",
    "print(len(dataset_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': '新华社受权于18日全文播发修改后的《中华人民共和国立法法》，修改后的立法法分为“总则”“法律”“行政法规”“地方性法规、自治条例和单行条例、规章”“适用与备案审查”“附则”等6章，共计105条。', 'summary': '修改后的立法法全文公布'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m d_len \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m dataset_train] \n\u001b[1;32m      2\u001b[0m s_len \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(s[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m dataset_train] \n\u001b[0;32m----> 3\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3.5\u001b[39m), sharey\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhist(d_len, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC0\u001b[39m\u001b[38;5;124m\"\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m      5\u001b[0m axes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDialogue Token Length\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "d_len = [len(tokenizer.encode(s['document'])) for s in dataset_train] \n",
    "s_len = [len(tokenizer.encode(s['summary'])) for s in dataset_train] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[0].set_title(\"Dialogue Token Length\") \n",
    "axes[0].set_xlabel(\"Length\") \n",
    "axes[0].set_ylabel(\"Count\") \n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[1].set_title(\"Summary Token Length\") \n",
    "axes[1].set_xlabel(\"Length\") \n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenModel = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(TokenModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [str(doc) for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    inputs = [str(doc) for doc in examples[\"summary\"]]\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(inputs, max_length=max_target_length, truncation=True)\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_t = dataset_train.map(preprocess_function, batched=True)\n",
    "tokenized_datasets_v = dataset_valid.map(preprocess_function, batched=True)\n",
    " \n",
    "tokenized_datasets = datasets.DatasetDict({\"train\":tokenized_datasets_t,\"validation\": tokenized_datasets_v})\n",
    "print(tokenized_datasets)\n",
    "\n",
    "print(tokenized_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed \n",
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12-final"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
