{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_features(example_batch): \n",
    "# \tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "# \twith tokenizer.as_target_tokenizer(): \n",
    "# \t\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "# \treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "# dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "# columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "# dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "def convert_examples_to_features(example_batch): \n",
    "\tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "\t# with tokenizer.as_target_tokenizer(): \n",
    "\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "\treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq \n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "\n",
    "#然后，像往常一样，我们为训练设置了一个TrainingArguments:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments( output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)\n",
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=dataset_samsum_pt[\"train\"], eval_dataset=dataset_samsum_pt[\"validation\"])\n",
    "\n",
    "trainer.train() \n",
    "score = evaluate_summaries_pegasus( dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\") \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) pd.DataFrame(rouge_dict, index=[f\"pegasus\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('bot-mvp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4632cd8377e743b0d90da04eeb35bbed8ec5d08da721a48d0317015ab0d939e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
