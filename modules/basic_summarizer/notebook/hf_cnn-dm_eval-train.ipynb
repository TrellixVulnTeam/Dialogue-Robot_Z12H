{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face eval cnn-dm\n",
    "+ 使用 cnn dailymail 通过 Hugging Face 的各个模型效果评估\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers\n",
    "+ https://github.com/hellotransformers/Natural_Language_Processing_with_Transformers/blob/main/chapter6.md\n",
    "+ https://xiaosheng.run/2022/03/29/transformers-note-8.html\n",
    "+ https://github.com/datawhalechina/learn-nlp-with-transformers/blob/main/docs/%E7%AF%87%E7%AB%A04-%E4%BD%BF%E7%94%A8Transformers%E8%A7%A3%E5%86%B3NLP%E4%BB%BB%E5%8A%A1/4.7-%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/bin/python\n"
     ]
    }
   ],
   "source": [
    "# 使用终端安装完包之后要重启jupyter\n",
    "!which python\n",
    "# !pip list\n",
    "!export http_proxy=\"http://172.19.57.45:3128/\"\n",
    "!export https_proxy=\"http://172.19.57.45:3128/\"\n",
    "# !export TRANSFORMERS_OFFLINE=1 # 强制离线加载模型\n",
    "# # !export TRANSFORMERS_OFFLINE=0 # 强制在线\n",
    "# !export HF_DATASETS_OFFLINE=1 # 强制离线加载数据\n",
    "# !export http_proxy=''\n",
    "# !export http_proxy=''\n",
    "# dataset 2.5.2\n",
    "# !pip install datasets==2.5.2\n",
    "# !pip install sacrebleu==2.3.1\n",
    "# rouge_score==0.0.4 work well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试gpu使用情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN DM 数据介绍\n",
    "+ CNN/DailyMail数据集由大约300,000对新闻文章及其相应的摘要组成，这些摘要由CNN和DailyMail在其文章中附加的要点组成\n",
    "+ 该数据集的一个重要方面是，摘要是抽象的，而不是摘录的，这意味着它们由新的句子而不是简单的摘录组成\n",
    "+ 该数据集可在Hub上找到；我们将使用3.0.0版本，这是一个为摘要而设置的非匿名版本\n",
    "+ 训练集大小： 286817\n",
    "+ 验证集大小： 13368\n",
    "+ 测试集大小： 11487\n",
    "+ 训练集中平均摘要句子数： 3.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备\n",
    "+ 因为网络或者代理的问题，数据从云端直接下载有问题，解决方案如下\n",
    "+ 远程加载 可以参考 https://github.com/huggingface/datasets/issues/996\n",
    "+ 本地加载 可以参考 https://blog.csdn.net/PolarisRisingWar/article/details/124042709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "# 代理必须关闭\n",
    "# 服务器上也需要关闭代理\n",
    "# hide_output\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 远程加载\n",
    "# dataset = load_dataset(\"cnn_dailymail\",  version=\"3.0.0\") # 有bug\n",
    "# dataset = load_dataset(\"ccdv/cnn_dailymail\",  version=\"3.0.0\")\n",
    "# 本地加载\n",
    "dataset = datasets.load_from_disk('hf_cnn-dm')\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该数据集有三列：文章，其中包含新闻文章，亮点与摘要，以及唯一标识每篇文章的ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Article (excerpt of 500 characters, total length: 3192):\n",
      "\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n",
      "\n",
      ">>> Summary (length: 180):\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    ">>> Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\n>>> Summary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has now collected eight gold medals at world championships, equaling the record held by American trio Carl Lewis, Michael Johnson and Allyson Felix, not to mention the small matter of six Olympic titles. The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital. \"I'm proud of myself and I'll continue to work to dominate for as long as possible,\" Bolt said, having previously expressed his intention to carry on until the 2016 Rio Olympics. Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade, while Gatlin and the United States third leg runner Rakieem Salaam had problems. Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt. Earlier, Jamaica's women underlined their dominance in the sprint events by winning the 4x100m relay gold, anchored by Shelly-Ann Fraser-Pryce, who like Bolt was completing a triple. Their quartet recorded a championship record of 41.29 seconds, well clear of France, who crossed the line in second place in 42.73 seconds. Defending champions, the United States, were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner, but promoted to silver when France were subsequently disqualified for an illegal handover. The British quartet, who were initially fourth, were promoted to the bronze which eluded their men's team. Fraser-Pryce, like Bolt ag\n"
     ]
    }
   ],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "print(sample_text)\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from transformers import set_seed \n",
    "from transformers import pipeline\n",
    "import torch\n",
    "set_seed(42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use bart in pytorch\n",
    "# from transformers import  pipeline\n",
    "# summarizer = pipeline(\"summarization\",proxy='')\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)\n",
    "\n",
    "# # use t5 in tf\n",
    "# summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
    "# summarizer(\"An apple a day, keeps the doctor away\", min_length=5, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_sentence_summary(text): \n",
    "\treturn \"\\n\".join(sent_tokenize(text)[:3]) \n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Olympic champion wins gold in Moscow.\n",
      "Bolt gets back in the picture with an impressive silver.\n",
      "A triple by Fraser-Pryce, Bailey-Cole, and Carter leaves the Jamaicans in the lead for the last part of the relay.\n",
      "Bolt dominates the final 50 meters and finishes off his hat trick and completes his double by completing the triple on the home straight, winning the gold with a world-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"../../../resources/embedding/gpt2-xl\")\n",
    "# pipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\" \n",
    "pipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n",
    "\n",
    "summaries[\"gpt2\"] = \"\\n\".join( sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n",
    "print(summaries[\"gpt2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/transformers/generation_utils.py:745: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usain bolt wins his third gold of the world championships in the men's 4x100m relay .\n",
      "the 26-year-old anchored the Jamaican quartet to victory in 37.36 seconds .\n",
      "he has now collected eight gold medals at the championships, equaling the record .\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/t5-large\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "print(summaries[\"t5\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/facebook_bart-large-cnn\") \n",
    "pipe_out = pipe(sample_text) \n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))\n",
    "print(summaries[\"bart\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usain Bolt wins third gold of world championships.\n",
      "Anchors Jamaica to victory in men's 4x100m relay.\n",
      "Eighth gold at the championships for Bolt.\n",
      "Jamaica also win women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    # pipe = pipeline(\"summarization\", model=\"../../../resources/embedding/google_pegasus-cnn-dailymail\") \n",
    "    pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\") \n",
    "    pipe_out = pipe(sample_text) \n",
    "    summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")\n",
    "print(summaries[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同模型效果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n",
      "\n",
      "BASELINE\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
      "\n",
      "GPT2\n",
      "The Olympic champion wins gold in Moscow.\n",
      "Bolt gets back in the picture with an impressive silver.\n",
      "A triple by Fraser-Pryce, Bailey-Cole, and Carter leaves the Jamaicans in the lead for the last part of the relay.\n",
      "Bolt dominates the final 50 meters and finishes off his hat trick and completes his double by completing the triple on the home straight, winning the gold with a world-\n",
      "\n",
      "T5\n",
      "usain bolt wins his third gold of the world championships in the men's 4x100m relay .\n",
      "the 26-year-old anchored the Jamaican quartet to victory in 37.36 seconds .\n",
      "he has now collected eight gold medals at the championships, equaling the record .\n",
      "\n",
      "BART\n",
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n",
      "\n",
      "PEGASUS\n",
      "Usain Bolt wins third gold of world championships.\n",
      "Anchors Jamaica to victory in men's 4x100m relay.\n",
      "Eighth gold at the championships for Bolt.\n",
      "Jamaica also win women's 4x100m relay .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\") \n",
    "print(dataset[\"train\"][1][\"highlights\"]) \n",
    "print(\"\") \n",
    "for model_name in summaries: \n",
    "\tprint(model_name.upper()) \n",
    "\tprint(summaries[model_name]) \n",
    "\tprint(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf (last modified on Tue Oct 25 00:02:35 2022) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric \n",
    "bleu_metric = load_metric(\"sacrebleu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[2, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[6, 5, 4, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[33.33, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Value\n",
       "score                            0\n",
       "counts                [2, 0, 0, 0]\n",
       "totals                [6, 5, 4, 3]\n",
       "precisions  [33.33, 0.0, 0.0, 0.0]\n",
       "bp                               1\n",
       "sys_len                          6\n",
       "ref_len                          6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "bleu_metric.add( prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"]) \n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>57.893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[5, 3, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[5, 4, 3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[100.0, 75.0, 66.67, 50.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>0.818731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Value\n",
       "score                           57.893\n",
       "counts                    [5, 3, 2, 1]\n",
       "totals                    [5, 4, 3, 2]\n",
       "precisions  [100.0, 75.0, 66.67, 50.0]\n",
       "bp                            0.818731\n",
       "sys_len                              5\n",
       "ref_len                              6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.add( prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n",
    "results = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0) \n",
    "results[\"precisions\"] = [np.round(p, 2)for p in results[\"precisions\"]] \n",
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/users/sunhongchao/.cache/huggingface/modules/datasets_modules/metrics/rouge/0ffdb60f436bdb8884d5e4d608d53dbe108e82dac4f494a66f80ef3f647c104f (last modified on Tue Oct 25 19:02:08 2022) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# rouge_score==0.0.4 work well\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.232143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.472222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.506329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.303571  0.090909  0.214286   0.232143\n",
       "gpt2      0.244898  0.000000  0.122449   0.224490\n",
       "t5        0.472222  0.228571  0.388889   0.472222\n",
       "bart      0.582278  0.207792  0.455696   0.506329\n",
       "pegasus   0.866667  0.655172  0.800000   0.833333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = dataset[\"train\"][1][\"highlights\"] \n",
    "records = [] \n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"] \n",
    "for model_name in summaries: \n",
    "\trouge_metric.add(prediction=summaries[model_name], reference=reference) \n",
    "\tscore = rouge_metric.compute() \n",
    "\trouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\trecords.append(rouge_dict) \n",
    "pd.DataFrame.from_records(records, index=summaries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用pegasus 抽样评估 测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summaries_baseline(dataset, metric, column_text=\"article\", column_summary=\"highlights\"): \n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]] \n",
    "    metric.add_batch(predictions=summaries, references=dataset[column_summary]) \n",
    "    score = metric.compute() \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at hf_cnn-dm/test/cache-f551f6b1b06308aa.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.170554</td>\n",
       "      <td>0.247146</td>\n",
       "      <td>0.354972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.388071  0.170554  0.247146   0.354972"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(1000)) \n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric) \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "def chunks(list_of_elements, batch_size): \n",
    "\t\"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\" \n",
    "\tfor i in range(0, len(list_of_elements), batch_size): \n",
    "\t    yield list_of_elements[i : i + batch_size] \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"highlights\"): \n",
    "\tarticle_batches = list(chunks(dataset[column_text], batch_size)) \n",
    "\ttarget_batches = list(chunks(dataset[column_summary], batch_size)) \n",
    "\tfor article_batch, target_batch in tqdm( zip(article_batches, target_batches), total=len(article_batches)): \n",
    "\t\tinputs = tokenizer(article_batch, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\") \n",
    "\t\tsummaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device), length_penalty=0.8, num_beams=8, max_length=128) \n",
    "\t\tdecoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries] \n",
    "\t\tdecoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries] \n",
    "\t\tmetric.add_batch(predictions=decoded_summaries, references=target_batch) \n",
    "\tscore = metric.compute() \n",
    "\treturn score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer \n",
    "\n",
    "# import torch\n",
    "# with torch.no_grad():\n",
    "#     model_ckpt = \"google/pegasus-cnn_dailymail\" \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_ckpt) \n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device) \n",
    "#     score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8) \n",
    "#     rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "#     pd.DataFrame(rouge_dict, index=[\"pegasus\"])\n",
    "\n",
    "# print(rouge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.38807104712508933, 'rouge2': 0.17055379457597958, 'rougeL': 0.2471461969561793, 'rougeLsum': 0.35497217427274075}\n"
     ]
    }
   ],
   "source": [
    "print(rouge_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练一个摘要模型\n",
    "+ 使用 SAMSum\n",
    "+ SAMSum 数据介绍如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "# dataset_samsum = load_dataset(\"samsum\") \n",
    "dataset_samsum = datasets.load_from_disk('hf_samsum')\n",
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum] \n",
    "print(f\"Split lengths: {split_lengths}\") \n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\") \n",
    "print(\"\\nDialogue:\") \n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"\\nSummary:\") \n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 流水线评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "/home/users/sunhongchao/miniconda3/envs/bot-mvp-3/lib/python3.6/site-packages/transformers/generation_utils.py:745: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  beam_id = beam_token_id // vocab_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    }
   ],
   "source": [
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"]) \n",
    "print(\"Summary:\") \n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，该模型大多试图通过提取对话中的关键句子来进行文本摘要。这在CNN/DailyMail数据集上可能效果相对较好，但SAMSum中的文本摘要更加抽象。让我们通过在测试集上运行完整的ROUGE评估来确认这一点:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f4305cb53a2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_summaries_pegasus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_samsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dialogue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrouge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrouge_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pegasus\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8) \n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果不是很好，但这并不意外，因为我们已经远离了CNN/DailyMail的数据分布。尽管如此，在训练前设置评估流水线有两个好处：我们可以直接用指标来衡量训练的成功与否，而且我们有一个好的基线。在我们的数据集上对模型进行微调，应该会使ROUGE指标立即得到改善，如果不是这样，我们就知道我们的训练循环出了问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调 pegasus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"] [\"dialogue\"]] \n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]] \n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n",
    "axes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[0].set_title(\"Dialogue Token Length\") \n",
    "axes[0].set_xlabel(\"Length\") \n",
    "axes[0].set_ylabel(\"Count\") \n",
    "axes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\") \n",
    "axes[1].set_title(\"Summary Token Length\") \n",
    "axes[1].set_xlabel(\"Length\") \n",
    "plt.tight_layout() \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到，大多数对话比CNN/DailyMail的文章短得多，每个对话有100-200个标记。同样，摘要也短得多，大约有20-40个符号（一条推文的平均长度）\n",
    "让我们在为训练者建立数据整理器时牢记这些意见。首先，我们需要对数据集进行标记。现在，我们将对话和摘要的最大长度分别设置为1024和128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_examples_to_features(example_batch): \n",
    "# \tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "# \twith tokenizer.as_target_tokenizer(): \n",
    "# \t\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "# \treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "# dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "# columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "# dataset_samsum_pt.set_format(type=\"torch\", columns=columns)\n",
    "\n",
    "def convert_examples_to_features(example_batch): \n",
    "\tinput_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True) \n",
    "\t# with tokenizer.as_target_tokenizer(): \n",
    "\ttarget_encodings = tokenizer(example_batch[\"summary\"], max_length=128, truncation=True) \n",
    "\treturn {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]} \n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True) \n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"] \n",
    "dataset_samsum_pt.set_format(type=\"torch\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用标记化步骤的一个新东西是tokenizer.as_target_tokenizer()上下文。有些模型在解码器输入中需要特殊的标记，所以区分编码器和解码器输入的标记很重要。在with语句（称为上下文管理器）中，标记器知道它正在为解码器进行标记，并可以相应地处理序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq \n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "\n",
    "#然后，像往常一样，我们为训练设置了一个TrainingArguments:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer \n",
    "training_args = TrainingArguments( output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500, per_device_train_batch_size=1, per_device_eval_batch_size=1, weight_decay=0.01, logging_steps=10, push_to_hub=True,\n",
    "evaluation_strategy='steps', eval_steps=500, save_steps=1e6, gradient_accumulation_steps=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer, data_collator=seq2seq_data_collator, train_dataset=dataset_samsum_pt[\"train\"], eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train() \n",
    "score = evaluate_summaries_pegasus( dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size=2, column_text=\"dialogue\", column_summary=\"summary\") \n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names) \n",
    "pd.DataFrame(rouge_dict, index=[f\"pegasus\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('bot-mvp-3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7376174dba2c0f0f332eb5a05ceaaa2df22c27cd567ee777b9932514a8161813"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
