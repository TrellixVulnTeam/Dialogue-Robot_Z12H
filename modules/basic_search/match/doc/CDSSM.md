# CDSSM
+ A latent semantic model with convolutional-pooling structure for information retrieval.
+ **针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层**
+ 模型结构
  + 输入层
    + 英文
      + 英文的处理方式，除了DSSM提到的 letter-trigram，CNN-DSSM 还在输入层增加了word-trigram  
      + ![20210814162716](https://i.loli.net/2021/08/14/MXZOj91835cSJQK.png)
      + 如上图所示，word-trigram其实就是一个包含了上下文信息的滑动窗口。举个例子：把<s> online auto body ... <s>这句话提取出前三个词<s> online auto，之后再分别对这三个词进行letter-trigram映射到一个 3 万维的向量空间里，然后把三个向量 concat 起来，最终映射到一个 9 万维的向量空间里。
    + 中文
      + 英文的处理方式（word-trigram letter-trigram）在中文中并不可取，因为英文中虽然用了 word-ngram 把样本空间拉成了百万级，但是经过 letter-trigram 又把向量空间降到可控级别，只有 3*30K（9 万）。而中文如果用 word-trigram，那向量空间就是百万级的了，显然还是字向量（1.5 万维）比较可控。
      + 简单来说，就是和DSSM 的中文处理方式一致
  + 表示层
    + ![20210814163057](https://i.loli.net/2021/08/14/92SLP8uA4TDBtom.png)
    + 卷积层
      + (英文)卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302*90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3*90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300*1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 +步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300*1 的 feature map 矩阵。
      + (中文) 302 * 15000
      + ![20210814163347](https://i.loli.net/2021/08/14/2kDdg4om53wyq7N.png)
    + 池化层
      + 池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300*1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。
    + 全连接层
      + 最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：$\tanh (x)=\frac{1-e^{-2 x}}{1+e^{-2 x}}$
  + 匹配层
    + 与DSSM 一致
+ 优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。
+ 缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

# 总结
**+ 1. DSSM 是端到端的模型，虽然省去了人工特征转化、特征工程和特征组合，但端到端的模型有个问题就是效果不可控。对于一些要保证较高的准确率的场景，用有监督人工标注的 query 分类作为打底，再结合无监督的 word2vec、LDA 等进行语义特征的向量化，显然比较可控（至少 query 分类的准确率可以达到 95%以上）。

+ 2. DSSM 是弱监督模型，因为引擎的点击曝光日志里 Query 和 Title 的语义信息比较弱。举个例子，搜索引擎第一页的信息往往都是 Query 的包含匹配，笔者统计过，完全的语义匹配只有不到 2%。这就意味着几乎所有的标题里都包含用户 Query 里的关键词，而仅用点击和曝光就能作为正负样例的判断？显然不太靠谱，因为大部分的用户进行点击时越靠前的点击的概率越大，而引擎的排序又是由 pCTR、CVR、CPC 等多种因素决定的。从这种非常弱的信号里提取出语义的相似性或者差别，那就需要有海量的训练样本。DSSM 论文中提到，实验的训练样本超过 1 亿。笔者和同事也亲测过，用传统 CTR 预估模型千万级的样本量来训练，模型无法收敛。可是这样海量的训练样本，恐怕只有搜索引擎才有吧？普通的搜索业务 query 有上千万，可资源顶多只有几百万，像论文中说需要挑出点击和曝光置信度比较高且资源热度也比较高的作为训练样本，这样就过滤了 80%的长尾 query 和 Title 结果对，所以也只有搜索引擎才有这样的训练语料了吧。另一方面，超过 1 亿的训练样本作为输入，用深度学习模型做训练，需要大型的 GPU 集群，这个对于很多业务来说也是不具备的条件。**

# Reference
+ https://www.cnblogs.com/wmx24/p/10157154.html