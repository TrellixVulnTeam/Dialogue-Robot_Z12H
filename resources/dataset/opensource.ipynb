{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face dataset https://huggingface.co/docs/datasets/loading\n",
    "# load text dataset https://huggingface.co/docs/datasets/nlp_load\n",
    "# https://huggingface.co/docs/datasets/nlp_process\n",
    "# https://huggingface.co/docs/datasets/process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2022-09-21 13:17:04,869] [ WARNING]\u001b[0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranetPlease import paddlenlp before datasets module to avoid download issues\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from paddlenlp.datasets import MapDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2022-09-21 13:17:09,808 builder.py:641] Reusing dataset msra_ner (/Users/sunhongchao/.cache/huggingface/datasets/msra_ner/msra_ner/1.0.0/5ce47bc7f8da59fd9d0ad08d185fa72f5576b614f136a56e82c7669d22ea5cfe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'paddlenlp.datasets.dataset.MapDataset'>\n",
      "{'id': '2', 'tokens': ['因', '有', '关', '日', '寇', '在', '京', '掠', '夺', '文', '物', '详', '情', '，', '藏', '界', '较', '为', '重', '视', '，', '也', '是', '我', '们', '收', '藏', '北', '京', '史', '料', '中', '的', '要', '件', '之', '一', '。'], 'ner_tags': [0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "hf_train_ds = load_dataset('msra_ner', split='train')\n",
    "print(type(hf_train_ds)) # <class 'datasets.arrow_dataset.Dataset'>\n",
    "\n",
    "train_ds = MapDataset(hf_train_ds)\n",
    "print(type(train_ds)) # <class 'paddlenlp.datasets.dataset.MapDataset'>\n",
    "\n",
    "print(train_ds[2]) # {'id': '2',\n",
    "                   #  'ner_tags': [0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   #               0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   #  'tokens': ['因', '有', '关', '日', '寇', '在', '京', '掠', '夺', '文', '物',\n",
    "                   #             '详', '情', '，', '藏', '界', '较', '为', '重', '视', '，', '也',\n",
    "                   #             '是', '我', '们', '收', '藏', '北', '京', '史', '料', '中', '的',\n",
    "                   #             '要', '件', '之', '一', '。']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2022-09-21 13:17:12,552 builder.py:463] Using custom data configuration default\n",
      "WARNING 2022-09-21 13:17:12,562 builder.py:641] Reusing dataset cmrc2018 (/Users/sunhongchao/.cache/huggingface/datasets/cmrc2018/default/0.1.0/3cbb788a586e4597f67937944006349cd758baef9409fb90a6ddb85c1c84690c)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'TRAIN_113_QUERY_0', 'context': '徐珂（），原名昌，字仲可，浙江杭县（今属杭州市）人。光绪举人。后任商务印书馆编辑。参加南社。1901年在上海担任了《外交报》、《东方杂志》的编辑，1911年，接管《东方杂志》的“杂纂部”。与潘仕成、王晋卿、王辑塘、冒鹤亭等友好。编有《清稗类钞》、《历代白话诗选》、《古今词选集评》等。光绪十五年（1889年）举人。后任商务印书馆编辑。参加南社。曾担任袁世凯在天津小站练兵时的幕僚，不久离去。', 'question': '徐珂字什么？', 'answers': {'text': ['字仲可'], 'answer_start': [9]}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hf_train_ds = load_dataset('cmrc2018', split='train')\n",
    "train_ds = MapDataset(hf_train_ds)\n",
    "print(train_ds[1818]) # {'answers': {'answer_start': [9], 'text': ['字仲可']},\n",
    "                      #  'context': '徐珂（），原名昌，字仲可，浙江杭县（今属杭州市）人。光绪举人。\n",
    "                      #              后任商务印书馆编辑。参加南社。1901年在上海担任了《外交报》、\n",
    "                      #              《东方杂志》的编辑，1911年，接管《东方杂志》的“杂纂部”。与潘仕成、\n",
    "                      #              王晋卿、王辑塘、冒鹤亭等友好。编有《清稗类钞》、《历代白话诗选》、\n",
    "                      #              《古今词选集评》等。光绪十五年（1889年）举人。后任商务印书馆编辑。\n",
    "                      #              参加南社。曾担任袁世凯在天津小站练兵时的幕僚，不久离去。',\n",
    "                      #  'id': 'TRAIN_113_QUERY_0',\n",
    "                      #  'question': '徐珂字什么？'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 2022-09-21 13:17:15,167 builder.py:641] Reusing dataset glue (/Users/sunhongchao/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "hf_train_ds = load_dataset('glue', 'sst2', split='train')\n",
    "train_ds = MapDataset(hf_train_ds)\n",
    "print(train_ds[0]) # {'idx': 0, 'label': 0, 'sentence': 'hide new secretions from the parental units '}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_train_ds = load_dataset('ptb_text_only', split='train')\n",
    "# train_ds = MapDataset(hf_train_ds)\n",
    "# print(train_ds[1]) # {'sentence': 'pierre <unk> N years old will join the board as a nonexecutive director nov. N'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 下载文件包并解压，解压文件夹在当前文件夹的datasets目录下\n",
    "# # 注意：datasets目录不需要新建，重复执行代码会自动检查文件是否存在，不会重复下载\n",
    "# file = tf.keras.utils.get_file(\n",
    "#         fname=\"cmrc2018.tar.gz\",\n",
    "#         origin=\"https://bj.bcebos.com/paddlehub-dataset/cmrc2018.tar.gz\",\n",
    "#         extract=True,\n",
    "#         cache_dir='.',\n",
    "#     )\n",
    "# # 文件路径\n",
    "# train_path = os.path.join(\".\", 'datasets/cmrc2018/cmrc2018_train.json')\n",
    "# eval_path = os.path.join(\".\", 'datasets/cmrc2018/cmrc2018_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 7.88kB [00:00, 1.77MB/s]                   \n",
      "Downloading metadata: 19.5kB [00:00, 4.41MB/s]                   \n",
      "WARNING 2022-09-21 13:21:04,773 builder.py:418] No config specified, defaulting to: kd_conv/all_dialogues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset kd_conv/all_dialogues (download: 10.53 MiB, generated: 15.07 MiB, post-processed: Unknown size, total: 25.60 MiB) to /Users/sunhongchao/.cache/huggingface/datasets/kd_conv/all_dialogues/0.0.0/7c75514efa08c2416d870f771e7f1d135a6e19944a02bfc9454c861b8b25a7f2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 11.0MB [00:31, 353kB/s] \n"
     ]
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Checksums didn't match for dataset source files:\n['https://github.com/thu-coai/KdConv/archive/master.zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sunhongchao/Documents/Bot/resources/dataset/load_text_data.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sunhongchao/Documents/Bot/resources/dataset/load_text_data.ipynb#ch0000007?line=0'>1</a>\u001b[0m hf_train_ds \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mkd_conv\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, download_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mforce_redownload\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunhongchao/Documents/Bot/resources/dataset/load_text_data.ipynb#ch0000007?line=1'>2</a>\u001b[0m train_ds \u001b[39m=\u001b[39m MapDataset(hf_train_ds)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sunhongchao/Documents/Bot/resources/dataset/load_text_data.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(train_ds[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/datasets/load.py:1746\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1742'>1743</a>\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1744'>1745</a>\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1745'>1746</a>\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1746'>1747</a>\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1747'>1748</a>\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1748'>1749</a>\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1749'>1750</a>\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1750'>1751</a>\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1751'>1752</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1753'>1754</a>\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1754'>1755</a>\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1755'>1756</a>\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/load.py?line=1756'>1757</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/datasets/builder.py:704\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=701'>702</a>\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mHF google storage unreachable. Downloading and preparing it from source\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=702'>703</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[0;32m--> <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=703'>704</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=704'>705</a>\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager, verify_infos\u001b[39m=\u001b[39;49mverify_infos, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=705'>706</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=706'>707</a>\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=707'>708</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/datasets/builder.py:1227\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=1225'>1226</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verify_infos):\n\u001b[0;32m-> <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=1226'>1227</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(dl_manager, verify_infos, check_duplicate_keys\u001b[39m=\u001b[39;49mverify_infos)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/datasets/builder.py:775\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=772'>773</a>\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=773'>774</a>\u001b[0m \u001b[39mif\u001b[39;00m verify_infos \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n\u001b[0;32m--> <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=774'>775</a>\u001b[0m     verify_checksums(\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=775'>776</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mdownload_checksums, dl_manager\u001b[39m.\u001b[39;49mget_recorded_sizes_checksums(), \u001b[39m\"\u001b[39;49m\u001b[39mdataset source files\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=776'>777</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=778'>779</a>\u001b[0m \u001b[39m# Build splits\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/builder.py?line=779'>780</a>\u001b[0m \u001b[39mfor\u001b[39;00m split_generator \u001b[39min\u001b[39;00m split_generators:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py:40\u001b[0m, in \u001b[0;36mverify_checksums\u001b[0;34m(expected_checksums, recorded_checksums, verification_name)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(bad_urls) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py?line=38'>39</a>\u001b[0m     error_msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mChecksums didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m for_verification_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py?line=39'>40</a>\u001b[0m     \u001b[39mraise\u001b[39;00m NonMatchingChecksumError(error_msg \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(bad_urls))\n\u001b[1;32m     <a href='file:///Users/sunhongchao/miniconda3/lib/python3.9/site-packages/datasets/utils/info_utils.py?line=40'>41</a>\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mAll the checksums matched successfully\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m for_verification_name)\n",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m: Checksums didn't match for dataset source files:\n['https://github.com/thu-coai/KdConv/archive/master.zip']"
     ]
    }
   ],
   "source": [
    "hf_train_ds = load_dataset('kd_conv', split='train', download_mode=\"force_redownload\")\n",
    "train_ds = MapDataset(hf_train_ds)\n",
    "print(train_ds[0]) # {'idx': 0, 'label': 0, 'sentence': 'hide new secretions from the parental units '}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d20345af9713ebff8c68be7a4bc786792663d50f698c1c39eb248438a08adbaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
